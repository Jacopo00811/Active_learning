{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from util_functions import set_global_seed, full_training_set_baseline, active_learning_loop, format_time, create_imbalanced_cifar10\n",
    "from analysis_functions import plot_al_performance_across_seeds\n",
    "import numpy as np\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select simulation config / parameters\n",
    "\n",
    "Define mini-config to run, but also include the full config.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin of Parameters ###\n",
    "\n",
    "## Dataset & Model Parameters ##\n",
    "dataset_name = \"CIFAR-10\" # \"CIFAR-10\" or \"MNIST\"\n",
    "imbalanced_dataset = True # Enable to create an imbalanced dataset (only for CIFAR-10)\n",
    "model_name = \"ResNet-18\" # Model to use for the simulation (only ResNet-18 supported)\n",
    "\n",
    "## Simulation Parameters ##\n",
    "save_results = True # Enable to save results to file\n",
    "relative_save_folder = \"./run_results\" # Define relative save folder\n",
    "\n",
    "## Model Hyperparameters ##\n",
    "pretrained_weights = True # Enable to use pretrained weights for the model\n",
    "TRAIN_VAL_RATIO = 0.8 # Ratio of training data to use for training (remaining is used for validation)\n",
    "EPOCHS = 8 # Number of epochs to train the model for each budget size\n",
    "BATCH_SIZE = 32 # Batch size for data loaders\n",
    "\n",
    "## Seed and Baseline Parameters ##\n",
    "seeds = [0, 1, 2, 3, 4] # Set random seeds to decrease uncertainty\n",
    "train_full_dataset_baseline = True # Enable to run the model with all labelled data to establish maximum performance baseline\n",
    "\n",
    "## Active Learning Algorithm Parameters ##\n",
    "AL_ALGORITHMS = {\n",
    "    'random': {\"active\": True},\n",
    "    'uncertainty': {\"active\": True},\n",
    "    'typiclust': {\"active\": True},\n",
    "    'margin': {\"active\": True}, \n",
    "    'entropy': {\"active\": True},\n",
    "    'badge': {\"active\": True},\n",
    "    'coreset': {\"active\": True},\n",
    "}\n",
    "\n",
    "## Budget Strategy Parameters ##\n",
    "BUDGET_STRATEGIES = {\n",
    "    1: {\"active\": True, \"initial_size\": 1000, \"query_size\": 250, \"num_al_iterations\": 10}, # Final size: 3500\n",
    "    2: {\"active\": True, \"initial_size\": 4000, \"query_size\": 500, \"num_al_iterations\": 10}, # Final size: 9000\n",
    "    3: {\"active\": False, \"initial_size\": 10000, \"query_size\": 1000, \"num_al_iterations\": 10}, # Final size: 20000\n",
    "    4: {\"active\": False, \"initial_size\": 22000, \"query_size\": 2000, \"num_al_iterations\": 10}, # Final size: 42000 (almost full training/validation set size of 50000)\n",
    "}\n",
    "\n",
    "# Mimic real-world frequency distribution for CIFAR-10 classes (if enabled)\n",
    "base_ratios = {\n",
    "    0: 1.0,    # common classes\n",
    "    1: 1.0,\n",
    "    2: 0.7,    # moderately common\n",
    "    3: 0.7,\n",
    "    4: 0.4,    # uncommon\n",
    "    5: 0.4,\n",
    "    6: 0.2,    # rare\n",
    "    7: 0.2,\n",
    "    8: 0.1,    # very rare\n",
    "    9: 0.1\n",
    "}\n",
    "\n",
    "### End of Parameters ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamically select device, and extract info from config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Device Selection ##\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu') # Set the device to CUDA / MPS if available, otherwise to CPU.\n",
    "\n",
    "## AL Algorithms Setup ##\n",
    "al_algorithms = [algo for algo, config in AL_ALGORITHMS.items() if config[\"active\"]] # Retrieve selected AL algorithms\n",
    "\n",
    "## Budget Strategies Setup ##\n",
    "budget_strategies = [num for num, config in BUDGET_STRATEGIES.items() if config[\"active\"]] # Retrieve selected budget strategies\n",
    "budget_initial_sizes = [BUDGET_STRATEGIES[num][\"initial_size\"] for num in budget_strategies] # Retrieve initial sizes for each budget strategy\n",
    "budget_query_sizes = [BUDGET_STRATEGIES[num][\"query_size\"] for num in budget_strategies] # Retrieve query sizes for each budget strategy\n",
    "budget_total_al_iterations = [BUDGET_STRATEGIES[num][\"num_al_iterations\"] for num in budget_strategies] # Retrieve total number of AL iterations for each budget strategy\n",
    "budget_final_sizes = [budget_initial_sizes[i] + budget_query_sizes[i] * budget_total_al_iterations[i] for i in range(len(budget_strategies))] # Calculate final sizes for each budget strategy\n",
    "\n",
    "# Check for overlap between budget strategies\n",
    "for i in range(len(budget_strategies)):\n",
    "    for j in range(i+1, len(budget_strategies)):\n",
    "        budget_final_sizes[i] <= budget_initial_sizes[j], f\"Overlap between budget strategies detected. Strategy {budget_strategies[i]} has final size {budget_final_sizes[i]} and strategy {budget_strategies[j]} has initial size {budget_initial_sizes[j]}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulation Saved Data Structure ##\n",
    "# Store configuration data\n",
    "simulation_data = {\n",
    "    \"config\": {\n",
    "        \"simulation\": {\n",
    "            \"save_results\": save_results,\"relative_save_folder\": relative_save_folder,\n",
    "            \"device\": str(device),\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"model_name\": model_name,\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"pretrained_weights\": pretrained_weights,\n",
    "            \"train_val_ratio\": TRAIN_VAL_RATIO,\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE\n",
    "        },\n",
    "        \"seed_and_baseline\": {\n",
    "            \"seeds\": seeds,\n",
    "            \"train_full_dataset_baseline\": train_full_dataset_baseline\n",
    "        },\n",
    "        \"active_learning\": {\n",
    "            \"al_algorithms\": al_algorithms,\n",
    "        },\n",
    "        \"budget\": {\n",
    "            \"budget_strategies\": budget_strategies,\n",
    "            \"budget_initial_sizes\": budget_initial_sizes,\n",
    "            \"budget_query_sizes\": budget_query_sizes,\n",
    "            \"budget_total_al_iterations\": budget_total_al_iterations,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Store runtime data\n",
    "simulation_data[\"runtimes\"] = {\n",
    "    \"simulation\": None,\n",
    "    \"seeds\": {f\"seed_{seed}\": None for seed in seeds},\n",
    "    \"full_dataset_baselines\": {\n",
    "        f\"seed_{seed}_full_dataset_baseline\": None\n",
    "        for seed in seeds\n",
    "    },\n",
    "    \"budget_strategies\": {\n",
    "        f\"seed_{seed}_budget_{strategy}\": None \n",
    "        for seed in seeds \n",
    "        for strategy in budget_strategies\n",
    "    },\n",
    "    \"training_and_al_algorithms\": {\n",
    "        f\"seed_{seed}_budget_{strategy}_{algo}\": {\"training\": None, \"al\": None}\n",
    "        for seed in seeds\n",
    "        for strategy in budget_strategies\n",
    "        for algo in al_algorithms\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize results data storage\n",
    "simulation_data[\"results\"] = {}\n",
    "\n",
    "## File Name Generation & Save Folder Creation ##\n",
    "file_name = ( # Construct the filename\n",
    "    f\"{dataset_name.lower()}_\"\n",
    "    f\"{model_name.lower()}_\"\n",
    "    f\"ptw{1 if pretrained_weights else 0}_\"\n",
    "    f\"tvr{int(TRAIN_VAL_RATIO*100)}_\"\n",
    "    f\"ep{EPOCHS}_\"\n",
    "    f\"bs{BATCH_SIZE}_\"\n",
    "    f\"sds{'-'.join(map(str, seeds))}_\"\n",
    "    f\"bl{1 if train_full_dataset_baseline else 0}_\"\n",
    "    f\"algo-{'-'.join([algo[:3] for algo in al_algorithms])}_\"\n",
    "    f\"bsr{'-'.join(map(str, budget_strategies))}_\"\n",
    "    f\"bis{'-'.join(map(str, budget_initial_sizes))}_\"\n",
    "    f\"bqs{'-'.join(map(str, budget_query_sizes))}_\"\n",
    "    f\"bni{'-'.join(map(str, budget_total_al_iterations))}\"\n",
    ")\n",
    "\n",
    "file_path = os.path.join(relative_save_folder, file_name) # Construct the file path\n",
    "\n",
    "os.makedirs(relative_save_folder, exist_ok=True) # Ensure the folder exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out full config overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= ACTIVE LEARNING SIMULATION CONFIGURATION =============\n",
      "\n",
      "DATASET & MODEL:\n",
      "- Dataset: CIFAR-10\n",
      "- Model: ResNet-18\n",
      "\n",
      "SIMULATION PARAMETERS:\n",
      "- Running on device: MPS\n",
      "- Save Results Enabled: True\n",
      "\n",
      "MODEL HYPERPARAMETERS:\n",
      "- Pretrained Weights Enabled: True\n",
      "- Training/Validation Split Ratio: 0.8\n",
      "- Epochs per Training Iteration: 8\n",
      "- Batch Size: 32\n",
      "\n",
      "SEEDS & BASELINE:\n",
      "- Seeds: [0, 1, 2, 3, 4]\n",
      "- Train Full Dataset Baseline Enabled: True\n",
      "\n",
      "ACTIVE LEARNING:\n",
      " - Active AL Algorithms: RANDOM, UNCERTAINTY, TYPICLUST, MARGIN, ENTROPY, BADGE, CORESET\n",
      "\n",
      "BUDGET STRATEGIES:\n",
      "  - Budget Strategy 1:\n",
      "    - Initial size: 1000 → Final size: 3500\n",
      "    - Query size: 250 | AL Iterations: 10\n",
      "  - Budget Strategy 2:\n",
      "    - Initial size: 4000 → Final size: 9000\n",
      "    - Query size: 500 | AL Iterations: 10\n",
      "\n",
      "EPOCHS BREAKDOWN (of varying dataset sizes):\n",
      "Total Epochs: 2,392\n",
      "├── Baseline: 40\n",
      "└── Training: 2,352\n",
      "\n",
      "SIMULATION STRUCTURE:\n",
      "Seeds: 5\n",
      "│\n",
      "├── Baselines: 1\n",
      "│   └── Training\n",
      "│       └── Epochs: 8\n",
      "│\n",
      "└── Budget Strategies: 2\n",
      "    └── AL-Algorithms: 7\n",
      "        └── Iterations: 11.0 (on average)\n",
      "            ├── AL (skips first iteration)\n",
      "            └── Training\n",
      "                └── Epochs: 8\n",
      "\n",
      "====================================================================\n"
     ]
    }
   ],
   "source": [
    "## Size Parameters used for Status Printing ##\n",
    "num_seeds = len(seeds)\n",
    "num_baselines = len(seeds) if train_full_dataset_baseline else 0\n",
    "num_strategies = len(budget_strategies)\n",
    "num_al_algorithms = len(al_algorithms)\n",
    "\n",
    "## Print Simulation Configuration ##\n",
    "print(\"============= ACTIVE LEARNING SIMULATION CONFIGURATION =============\")\n",
    "\n",
    "print(\"\\nDATASET & MODEL:\")\n",
    "print(f\"- Dataset: {dataset_name}\")\n",
    "print(f\"- Model: {model_name}\")\n",
    "\n",
    "print(\"\\nSIMULATION PARAMETERS:\")\n",
    "print(\"- Running on device:\", str(device).upper())\n",
    "print(f\"- Save Results Enabled: {save_results}\")\n",
    "\n",
    "print(f\"\\nMODEL HYPERPARAMETERS:\")\n",
    "print(f\"- Pretrained Weights Enabled: {pretrained_weights}\")\n",
    "print(f\"- Training/Validation Split Ratio: {TRAIN_VAL_RATIO}\")\n",
    "print(f\"- Epochs per Training Iteration: {EPOCHS}\")\n",
    "print(f\"- Batch Size: {BATCH_SIZE}\")\n",
    "\n",
    "print(f\"\\nSEEDS & BASELINE:\")\n",
    "print(f\"- Seeds: {seeds}\")\n",
    "print(f\"- Train Full Dataset Baseline Enabled: {train_full_dataset_baseline}\")\n",
    "\n",
    "print(f\"\\nACTIVE LEARNING:\")\n",
    "print(f\" - Active AL Algorithms: {', '.join(al_algorithms).upper()}\")\n",
    "\n",
    "print(f\"\\nBUDGET STRATEGIES:\")\n",
    "for i, strategy in enumerate(budget_strategies):\n",
    "    print(f\"  - Budget Strategy {strategy}:\")\n",
    "    print(f\"    - Initial size: {budget_initial_sizes[i]} → Final size: {budget_final_sizes[i]}\")\n",
    "    print(f\"    - Query size: {budget_query_sizes[i]} | AL Iterations: {budget_total_al_iterations[i]}\")\n",
    "\n",
    "print(\"\"\"\n",
    "EPOCHS BREAKDOWN (of varying dataset sizes):\n",
    "Total Epochs: {total:,}\n",
    "├── Baseline: {baseline:,}\n",
    "└── Training: {training:,}\"\"\".format(\n",
    "   total=EPOCHS * (num_baselines + num_strategies*num_al_algorithms*(sum(budget_total_al_iterations)+1)),\n",
    "   baseline=EPOCHS * num_baselines,\n",
    "   training=EPOCHS * num_strategies*num_al_algorithms*(sum(budget_total_al_iterations)+1)\n",
    "))\n",
    "\n",
    "print(f\"\"\"\n",
    "SIMULATION STRUCTURE:\n",
    "Seeds: {len(seeds)}\n",
    "│\n",
    "├── Baselines: {1 if train_full_dataset_baseline else 0}\n",
    "│   └── Training\n",
    "│       └── Epochs: {EPOCHS if train_full_dataset_baseline else 0}\n",
    "│\n",
    "└── Budget Strategies: {len(budget_strategies)}\n",
    "    └── AL-Algorithms: {len(al_algorithms)}\n",
    "        └── Iterations: {(np.mean(budget_total_al_iterations) + 1)} (on average)\n",
    "            ├── AL (skips first iteration)\n",
    "            └── Training\n",
    "                └── Epochs: {EPOCHS}\n",
    "\"\"\")\n",
    "\n",
    "print(\"====================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for save results and setup dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check if both the file already exists and saving is enabled ##\n",
    "if save_results:\n",
    "    if os.path.exists(file_path): # Check if file already exists\n",
    "        raise FileExistsError(f\"Simulation {file_name} already exists in {file_path}\\nSimulation with current config already has saved results from a previous run.\\nIdentical config = Identical results, check saved simulation if data is needed.\\nIf you want to run the simulation again, change the configuration, delete the existing file, or alternatively set save_results to False.\")\n",
    "\n",
    "## Load the dataset and split into training and validation sets ##\n",
    "if dataset_name == \"CIFAR-10\":\n",
    "    # Load CIFAR-10 with transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    # CIFAR-10 Dataset (Training and Test splits)\n",
    "    train_val_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "elif dataset_name == \"MNIST\":\n",
    "    # Load MNIST with transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # MNIST specific normalization values\n",
    "    ])\n",
    "\n",
    "    # MNIST Dataset (Training and Test splits)\n",
    "    train_val_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "else:\n",
    "    raise ValueError(f\"Dataset {dataset_name} not supported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the full sim, explain the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Config summary:\n",
      "- Save results: True\n",
      "- Device: mps | Dataset: CIFAR-10\n",
      "- Model: ResNet-18 | Pretrained: True | Train/Val Ratio: 0.8 | Epochs: 8 | Batch Size: 32\n",
      "- Seeds to run: [0, 1, 2, 3, 4]\n",
      "- Run full dataset baseline per seed: True\n",
      "- Budget strategies per seed: [1, 2]\n",
      "- AL-algorithms per budget strategy: ['random', 'uncertainty', 'typiclust', 'margin', 'entropy', 'badge', 'coreset']\n",
      "\n",
      "Beginning AL Simulation...\n",
      "\n",
      "\n",
      "[SEED 0 (1/5)]\n",
      "Imbalanced CIFAR-10 created: 25300 samples kept out of 50000\n",
      "\n",
      "Class distribution:\n",
      "Class 0: 5000/5000 (100.0%)\n",
      "Class 1: 5000/5000 (100.0%)\n",
      "Class 2: 3744/5000 (74.9%)\n",
      "Class 3: 4060/5000 (81.2%)\n",
      "Class 4: 2466/5000 (49.3%)\n",
      "Class 5: 1755/5000 (35.1%)\n",
      "Class 6: 1237/5000 (24.7%)\n",
      "Class 7: 962/5000 (19.2%)\n",
      "Class 8: 474/5000 (9.5%)\n",
      "Class 9: 602/5000 (12.0%)\n",
      "Imbalanced CIFAR-10 training / validation created, len(train_val_dataset): 25300\n",
      "Test dataset is unchanged, len(test_dataset): 10000\n",
      "\n",
      "[SEED 0 (1/5) | TRAINING FULL DATASET BASELINE]\n",
      "Baseline Complete - Test: 72.84% acc, Total time: 1m 38.02s\n",
      "\n",
      "[SEED 0 (1/5)) | BUDGET STRATEGY 1 (1/2)]\n",
      "Initial size: 1000 → Final size: 3500 | Query size: 250 | AL Iterations: 10\n",
      "\n",
      "[SEED 0 (1/5) | BUDGET STRATEGY 1 (1/2) | AL: RANDOM (1/7)]\n",
      "  Iteration 0/10 - Samples: 1000 (20% val), Test: 46.74% acc, Time: 6.97s, Training: 3.93s, AL: N/A\n",
      "  Iteration 1/10 - Samples: 1250 (20% val), Test: 43.54% acc, Time: 8.03s, Training: 5.01s, AL: 0.00s\n",
      "  Iteration 2/10 - Samples: 1500 (20% val), Test: 48.10% acc, Time: 8.99s, Training: 5.85s, AL: 0.00s\n",
      "  Iteration 3/10 - Samples: 1750 (20% val), Test: 47.31% acc, Time: 9.83s, Training: 6.87s, AL: 0.00s\n",
      "  Iteration 4/10 - Samples: 2000 (20% val), Test: 52.51% acc, Time: 10.52s, Training: 7.46s, AL: 0.00s\n",
      "  Iteration 5/10 - Samples: 2250 (20% val), Test: 49.14% acc, Time: 11.60s, Training: 8.49s, AL: 0.00s\n",
      "  Iteration 6/10 - Samples: 2500 (20% val), Test: 49.08% acc, Time: 12.57s, Training: 9.46s, AL: 0.00s\n",
      "  Iteration 7/10 - Samples: 2750 (20% val), Test: 52.63% acc, Time: 13.31s, Training: 10.27s, AL: 0.00s\n",
      "  Iteration 8/10 - Samples: 3000 (20% val), Test: 51.89% acc, Time: 14.12s, Training: 11.15s, AL: 0.00s\n",
      "  Iteration 9/10 - Samples: 3250 (20% val), Test: 53.34% acc, Time: 15.31s, Training: 12.24s, AL: 0.00s\n",
      "  Iteration 10/10 - Samples: 3500 (20% val), Test: 54.81% acc, Time: 16.14s, Training: 13.11s, AL: 0.00s\n",
      "RANDOM complete - Test: 54.81% acc, Total training time: 1m 33.84s, Total AL time: 0.01s\n",
      "\n",
      "[SEED 0 (1/5) | BUDGET STRATEGY 1 (1/2) | AL: UNCERTAINTY (2/7)]\n",
      "  Iteration 0/10 - Samples: 1000 (20% val), Test: 46.74% acc, Time: 6.88s, Training: 3.85s, AL: N/A\n",
      "  Iteration 1/10 - Samples: 1250 (20% val), Test: 47.03% acc, Time: 13.22s, Training: 4.78s, AL: 5.40s\n",
      "  Iteration 2/10 - Samples: 1500 (20% val), Test: 50.37% acc, Time: 13.98s, Training: 5.67s, AL: 5.30s\n",
      "  Iteration 3/10 - Samples: 1750 (20% val), Test: 49.64% acc, Time: 15.19s, Training: 6.74s, AL: 5.27s\n",
      "\u001b[2KEpoch 8/8, Validation: 52.50% acc\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[SEED \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_seeds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) | BUDGET STRATEGY \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstrategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstrategy_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_strategies\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) | AL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mal_algorithm\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mal_algorithm_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_al_algorithms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Run active learning loop, return training set sizes and corresponding test accuracies\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m train_val_set_sizes, test_accuracies, training_time_total, al_time_total \u001b[38;5;241m=\u001b[39m \u001b[43mactive_learning_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_val_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTRAIN_VAL_RATIO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_val_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_val_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbudget_initial_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbudget_initial_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbudget_query_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbudget_query_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbudget_al_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbudget_al_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mal_algorithm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mal_algorithm\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Store results\u001b[39;00m\n\u001b[1;32m    115\u001b[0m simulation_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbudget_strategy_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstrategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m][al_algorithm][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_val_set_sizes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m train_val_set_sizes\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Master Study/1. Semester/02456 Deep Learning/Final project/Active_learning/util_functions.py:641\u001b[0m, in \u001b[0;36mactive_learning_loop\u001b[0;34m(device, dataset_name, model_name, pretrained_weights, epochs, batch_size, train_val_ratio, train_val_dataset, test_dataset, generator, budget_initial_size, budget_query_size, budget_al_iterations, al_algorithm)\u001b[0m\n\u001b[1;32m    638\u001b[0m training_time_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m training_time_iter\n\u001b[1;32m    640\u001b[0m \u001b[38;5;66;03m# Evaluate model on full test dataset\u001b[39;00m\n\u001b[0;32m--> 641\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_test_loader\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;66;03m# Store test accuracy and training + validation set sizes\u001b[39;00m\n\u001b[1;32m    648\u001b[0m test_accuracies\u001b[38;5;241m.\u001b[39mappend(test_accuracy)\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Master Study/1. Semester/02456 Deep Learning/Final project/Active_learning/util_functions.py:160\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(device, model, data_loader)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m    159\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 160\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    162\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.10/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.10/site-packages/torchvision/models/resnet.py:273\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    270\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m    271\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[0;32m--> 273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[1;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.10/site-packages/torchvision/models/resnet.py:94\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     92\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[1;32m     93\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[0;32m---> 94\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out)\n\u001b[1;32m     97\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.10/site-packages/torch/nn/modules/activation.py:133\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.10/site-packages/torch/nn/functional.py:1702\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1700\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(relu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39minplace)\n\u001b[1;32m   1701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m-> 1702\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1704\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Print Configuration Summary ##\n",
    "print(f\"\\nConfig summary:\")\n",
    "print(f\"- Save results: {save_results}\")\n",
    "print(f\"- Device: {device} | Dataset: {dataset_name}\")\n",
    "print(f\"- Model: {model_name} | Pretrained: {pretrained_weights} | Train/Val Ratio: {TRAIN_VAL_RATIO} | Epochs: {EPOCHS} | Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"- Seeds to run: {seeds}\")\n",
    "print(f\"- Run full dataset baseline per seed: {train_full_dataset_baseline}\")\n",
    "print(f\"- Budget strategies per seed: {budget_strategies}\")\n",
    "print(f\"- AL-algorithms per budget strategy: {al_algorithms}\")\n",
    "\n",
    "print(\"\\nBeginning AL Simulation...\")\n",
    "\n",
    "\n",
    "## Simulation Loop ##\n",
    "simulation_time = time.time() # Initialize timer\n",
    "\n",
    "# Run the active learning evaluation loop for each seed\n",
    "for seed_idx, seed in enumerate(seeds):\n",
    "    seed_time = time.time()\n",
    "    simulation_data[\"results\"][f\"seed_{seed}\"] = {}\n",
    "    print(f\"\\n\\n[SEED {seed} ({seed_idx+1}/{num_seeds})]\")\n",
    "    \n",
    "    # Set random seed and generator used for data loaders\n",
    "    initial_generator = set_global_seed(seed)\n",
    "\n",
    "    # Create imbalanced CIFAR-10 dataset\n",
    "    if imbalanced_dataset and dataset_name == \"CIFAR-10\":\n",
    "        # Add small random variations to make it more realistic\n",
    "        keep_ratios = {k: min(1.0, max(0.05, v + np.random.normal(0, 0.05))) \n",
    "                for k, v in base_ratios.items()}\n",
    "        \n",
    "        train_val_dataset = create_imbalanced_cifar10(train_val_dataset, keep_ratios)\n",
    "        print(\"Imbalanced CIFAR-10 training / validation created, len(train_val_dataset):\", len(train_val_dataset))\n",
    "        print(f\"Test dataset is unchanged, len(test_dataset): {len(test_dataset)}\")\n",
    "    else:\n",
    "        print(\"Imbalanced dataset disabled, skipping...\")\n",
    "\n",
    "    # Split into training and validation sets\n",
    "    train_size = int(TRAIN_VAL_RATIO * len(train_val_dataset))\n",
    "    val_size = len(train_val_dataset) - train_size\n",
    "\n",
    "    # Split the dataset into training and validation\n",
    "    train_dataset, val_dataset = random_split(train_val_dataset, [train_size, val_size], generator=initial_generator)\n",
    "\n",
    "    # (If enabled) Train the model with entire dataset labelled to establish maximum performance baseline\n",
    "    if train_full_dataset_baseline:\n",
    "        print(f\"\\n[SEED {seed} ({seed_idx+1}/{num_seeds}) | TRAINING FULL DATASET BASELINE]\")\n",
    "\n",
    "        full_dataset_baseline_time = time.time()\n",
    "\n",
    "        test_accuracy = full_training_set_baseline(\n",
    "            device=device, \n",
    "            dataset_name=dataset_name,\n",
    "            model_name=model_name, \n",
    "            pretrained_weights=pretrained_weights, \n",
    "            epochs=EPOCHS, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            train_dataset=train_dataset, \n",
    "            val_dataset=val_dataset, \n",
    "            test_dataset=test_dataset, \n",
    "            generator=initial_generator)\n",
    "\n",
    "        full_dataset_baseline_time = time.time() - full_dataset_baseline_time\n",
    "\n",
    "        simulation_data[\"results\"][f\"seed_{seed}\"][\"full_dataset_baseline\"] = test_accuracy\n",
    "        simulation_data[\"runtimes\"][\"full_dataset_baselines\"][f\"seed_{seed}_full_dataset_baseline\"] = full_dataset_baseline_time\n",
    "\n",
    "        print(f\"Baseline Complete - Test: {test_accuracy:.2f}% acc, Total time: {format_time(full_dataset_baseline_time)}\")\n",
    "    else:\n",
    "        simulation_data[\"results\"][f\"seed_{seed}\"][\"full_dataset_baseline\"] = None\n",
    "        simulation_data[\"runtimes\"][\"full_dataset_baselines\"][f\"seed_{seed}_full_dataset_baseline\"] = None\n",
    "        print(\"Full dataset baseline disabled, skipping...\")\n",
    "\n",
    "\n",
    "    # For each budget strategy, train, evaluate, visualize and compare all AL strategies\n",
    "    for strategy_idx, strategy in enumerate(budget_strategies):\n",
    "        budget_strategy_time = time.time()\n",
    "        simulation_data[\"results\"][f\"seed_{seed}\"][f\"budget_strategy_{strategy}\"] = {}\n",
    "\n",
    "        budget_initial_size = budget_initial_sizes[strategy_idx]\n",
    "        budget_query_size = budget_query_sizes[strategy_idx]\n",
    "        budget_al_iterations = budget_total_al_iterations[strategy_idx]\n",
    "        budget_final_size = budget_final_sizes[strategy_idx]\n",
    "\n",
    "        print(f\"\\n[SEED {seed} ({seed_idx+1}/{num_seeds})) | BUDGET STRATEGY {strategy} ({strategy_idx+1}/{num_strategies})]\")\n",
    "        print(f\"Initial size: {budget_initial_size} → Final size: {budget_final_size} | Query size: {budget_query_size} | AL Iterations: {budget_al_iterations}\")\n",
    "        \n",
    "        # Debug lists for initial test accuracy, should be the same for all AL algorithms since no AL happens in the first (0'th) iteration, only training on the same initial labelled set\n",
    "        debug_initial_test_accuracy = []\n",
    "\n",
    "        for al_algorithm_idx, al_algorithm in enumerate(al_algorithms):\n",
    "            simulation_data[\"results\"][f\"seed_{seed}\"][f\"budget_strategy_{strategy}\"][al_algorithm] = {}\n",
    "            \n",
    "            print(f\"\\n[SEED {seed} ({seed_idx+1}/{num_seeds}) | BUDGET STRATEGY {strategy} ({strategy_idx+1}/{num_strategies}) | AL: {al_algorithm.upper()} ({al_algorithm_idx+1}/{num_al_algorithms})]\")\n",
    "            \n",
    "            # Run active learning loop, return training set sizes and corresponding test accuracies\n",
    "            train_val_set_sizes, test_accuracies, training_time_total, al_time_total = active_learning_loop(\n",
    "                device=device, \n",
    "                dataset_name=dataset_name,\n",
    "                model_name=model_name,\n",
    "                pretrained_weights=pretrained_weights,\n",
    "                epochs=EPOCHS,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                train_val_ratio=TRAIN_VAL_RATIO,  \n",
    "                train_val_dataset=train_val_dataset,\n",
    "                test_dataset=test_dataset,\n",
    "                generator=initial_generator,\n",
    "                budget_initial_size=budget_initial_size,\n",
    "                budget_query_size=budget_query_size,\n",
    "                budget_al_iterations=budget_al_iterations,\n",
    "                al_algorithm=al_algorithm\n",
    "            )\n",
    "\n",
    "            # Store results\n",
    "            simulation_data[\"results\"][f\"seed_{seed}\"][f\"budget_strategy_{strategy}\"][al_algorithm][\"train_val_set_sizes\"] = train_val_set_sizes\n",
    "            simulation_data[\"results\"][f\"seed_{seed}\"][f\"budget_strategy_{strategy}\"][al_algorithm][\"test_accuracies\"] = test_accuracies\n",
    "\n",
    "            # Store training and AL algorithm runtimes\n",
    "            simulation_data[\"runtimes\"][\"training_and_al_algorithms\"][f\"seed_{seed}_budget_{strategy}_{al_algorithm}\"][\"training\"] = training_time_total\n",
    "            simulation_data[\"runtimes\"][\"training_and_al_algorithms\"][f\"seed_{seed}_budget_{strategy}_{al_algorithm}\"][\"al\"] = al_time_total\n",
    "\n",
    "            print(f\"{al_algorithm.upper()} complete - Test: {test_accuracies[-1]:.2f}% acc, Total training time: {format_time(training_time_total)}, Total AL time: {format_time(al_time_total)}\")\n",
    "\n",
    "            # Debug: Store initial test accuracy for comparison\n",
    "            debug_initial_test_accuracy.append(test_accuracies[0])\n",
    "            if len(debug_initial_test_accuracy) > 1:\n",
    "                assert abs(debug_initial_test_accuracy[-1] - debug_initial_test_accuracy[-2]) < 1e-6, \"Initial test accuracy should be the same for all AL algorithms, as no AL happens in the first iteration and model is trained on the same initial labelled set.\"\n",
    "\n",
    "\n",
    "        budget_strategy_time = time.time() - budget_strategy_time\n",
    "        simulation_data[\"runtimes\"][\"budget_strategies\"][f\"seed_{seed}_budget_{strategy}\"] = budget_strategy_time\n",
    "\n",
    "        strategy_best_algorithm = None\n",
    "        strategy_best_accuracy = 0\n",
    "\n",
    "        print(f\"\\n[SEED {seed} ({seed_idx+1}/{num_seeds})) | BUDGET STRATEGY {strategy} ({strategy_idx+1}/{num_strategies}) | COMPLETED]\")\n",
    "        print(f\"Total Runtime: {format_time(budget_strategy_time)}, Summary:\")\n",
    "        for al_algorithm_idx, al_algorithm in enumerate(al_algorithms):\n",
    "            test_accuracies = simulation_data[\"results\"][f\"seed_{seed}\"][f\"budget_strategy_{strategy}\"][al_algorithm][\"test_accuracies\"]\n",
    "            training_time_total = simulation_data[\"runtimes\"][\"training_and_al_algorithms\"][f\"seed_{seed}_budget_{strategy}_{al_algorithm}\"][\"training\"]\n",
    "            al_time_total = simulation_data[\"runtimes\"][\"training_and_al_algorithms\"][f\"seed_{seed}_budget_{strategy}_{al_algorithm}\"][\"al\"]\n",
    "\n",
    "            if test_accuracies[-1] > strategy_best_accuracy or strategy_best_algorithm is None:\n",
    "                strategy_best_algorithm = al_algorithm\n",
    "                strategy_best_accuracy = test_accuracies[-1]\n",
    "\n",
    "            print(f\"{al_algorithm_idx+1}. {al_algorithm.upper()} summary - Test: {test_accuracies[-1]:.2f}% acc, Total training time: {format_time(training_time_total)}, Total AL time: {format_time(al_time_total)}\")\n",
    "        print(f\"Best AL-Algorithm for Budget Strategy {strategy}: {strategy_best_algorithm.upper()}, Test Accuracy: {strategy_best_accuracy:.2f}%\")\n",
    "\n",
    "        plt.figure()\n",
    "        for al_algorithm in al_algorithms:\n",
    "            train_val_set_size = simulation_data[\"results\"][f\"seed_{seed}\"][f\"budget_strategy_{strategy}\"][al_algorithm][\"train_val_set_sizes\"]\n",
    "            test_accuracies = simulation_data[\"results\"][f\"seed_{seed}\"][f\"budget_strategy_{strategy}\"][al_algorithm][\"test_accuracies\"]\n",
    "            plt.plot(train_val_set_sizes, test_accuracies, label=f\"{al_algorithm.upper()}\\nFinal: {test_accuracies[-1]:.2f}% acc\",marker='o')\n",
    "        plt.xlabel(f'Training Set Size ({(1 - TRAIN_VAL_RATIO) * 100:.0f}% for Validation)')\n",
    "        plt.ylabel('Test Accuracy (%)')\n",
    "        plt.title(f'Active Learning: Test Accuracy vs. Labelled Set Size\\nSeed {seed} | Budget Strategy {strategy} | Best AL-Algorithm: {strategy_best_algorithm.upper()}')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    seed_time = time.time() - seed_time\n",
    "    simulation_data[\"runtimes\"][\"seeds\"][f\"seed_{seed}\"] = seed_time\n",
    "\n",
    "simulation_time = time.time() - simulation_time\n",
    "simulation_data[\"runtimes\"][\"simulation\"] = simulation_time\n",
    "\n",
    "# Plot average results for all AL algorithms for each budget strategy over all seeds\n",
    "print(\"SIMULATION COMPLETE\")\n",
    "print(f\"Total Runtime: {format_time(simulation_time)}\")\n",
    "if len(seeds) > 1:\n",
    "    print(\"\\nPlotting average AL-algorithm performances for each budget strategy across all seeds...\")\n",
    "    for strategy in budget_strategies:\n",
    "        fig = plot_al_performance_across_seeds(\n",
    "            simulation_data,\n",
    "            strategy,\n",
    "            al_algorithms\n",
    "        )\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"\\nSkipping average AL-algorithm performance plots - Only one seed was run.\")\n",
    "\n",
    "\n",
    "# Save the results to a JSON file if enabled and the file does not already exist\n",
    "if save_results:\n",
    "    if not os.path.exists(file_path): # Check if file already exists\n",
    "        # Save the dictionary as a JSON file\n",
    "        with open(file_path, \"w\") as file:\n",
    "            json.dump(simulation_data, file, indent=4)\n",
    "        print(f\"\\nSimulation results saved to \\n{file_path}\")\n",
    "else:\n",
    "    print(\"\\nSAVING DISABLED - Results not saved to file. Set save_results to True to save results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain what results you can expect, and the results seen here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
